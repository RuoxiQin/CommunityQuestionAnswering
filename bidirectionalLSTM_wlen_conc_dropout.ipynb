{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized BLSTM model on Answer Selection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_length = 100\n",
    "vector_size = 300\n",
    "batch_size = 16\n",
    "hidden_units1 = 64\n",
    "hidden_units2 = 64\n",
    "learning_rate = 0.001\n",
    "droprate_output = 0.8\n",
    "droprate_state = 1.0\n",
    "drop_rate = 0.5\n",
    "augment_feature_num = 10\n",
    "dense_units1 = 128\n",
    "dense_units2 = 128\n",
    "reg_coefficient = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "You can impelment whatever you want, just make sure to have the *has_next* and *get_batch* function with the corresponding output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, data_dir):\n",
    "        '''Initialization'''\n",
    "        npz_data = np.load(data_dir)\n",
    "        names = sorted(npz_data.files)\n",
    "        self._data = []\n",
    "        for name in names:\n",
    "            self._data.append(npz_data[name])\n",
    "        self._num_examples = self._data[0].shape[0]\n",
    "\n",
    "    def shuffle_data(self, idx):\n",
    "        for i in range(len(self._data)):\n",
    "            self._data[i]=self._data[i][idx]\n",
    "\n",
    "    def get_data(self, start, end):\n",
    "        res=[]\n",
    "        for i in range(len(self._data)):\n",
    "            res.append(self._data[i][start : end])\n",
    "        return tuple(res)\n",
    "    \n",
    "    def init_epoch(self, batch_size, shuffle=True):\n",
    "        self._index_in_epoch = 0\n",
    "        idx = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx)  # shuffle indexes\n",
    "        self.shuffle_data(idx)  # get list of `num` random samples\n",
    "        if batch_size <= 0:\n",
    "            self.batch_size = self._num_examples\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "    def has_next(self):\n",
    "        '''return bool: whether there is a next batch'''\n",
    "        return self._index_in_epoch < self._num_examples\n",
    "\n",
    "    def get_batch(self):\n",
    "        '''\n",
    "        return the next batch in the following tuple format:\n",
    "        (batch_input_q, batch_input_a, augmented_data, score_label)\n",
    "        Where\n",
    "        batch_input_q: word2vec representation for the question in shape [batch_size, sentence_length, vector_size]\n",
    "        batch_input_a: word2vec representation for the answer in shape [batch_size, sentence_length, vector_size]\n",
    "        batch_q_len: [batch_size]\n",
    "        batch_a_len: [batch_size]\n",
    "        augmented_data: the extra data for MLP in shape [batch_size, augment_feature_num]\n",
    "        score_label: ground truth semantic similarity score in shape [batch_size, 1]\n",
    "        comment_id: the commet id\n",
    "        '''\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += self.batch_size\n",
    "        end = self._index_in_epoch\n",
    "        return self.get_data(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholders\n",
    "batch_input_q = tf.placeholder(dtype=tf.float32, shape=(None, sentence_length, vector_size))\n",
    "batch_input_a = tf.placeholder(dtype=tf.float32, shape=(None, sentence_length, vector_size))\n",
    "batch_input_q_len = tf.placeholder(dtype=tf.int32, shape=(None,))\n",
    "batch_input_a_len = tf.placeholder(dtype=tf.int32, shape=(None,))\n",
    "score_label = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "enable_dropout = tf.placeholder(dtype=tf.bool, shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 bidirectional lstm layers\n",
    "def blstm_func(input_batch, sequence_length, init_states=((None, None), (None, None))):\n",
    "    with tf.variable_scope(\"blstm1\"):\n",
    "        cell_fw1 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units1, name='forward')\n",
    "        cell_fw1 = tf.nn.rnn_cell.DropoutWrapper(cell_fw1, output_keep_prob=droprate_output, state_keep_prob=droprate_state, variational_recurrent=True, dtype=tf.float32)\n",
    "        cell_bw1 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units1, name='backward')\n",
    "        cell_bw1 = tf.nn.rnn_cell.DropoutWrapper(cell_bw1, output_keep_prob=droprate_output, state_keep_prob=droprate_state, variational_recurrent=True, dtype=tf.float32)\n",
    "        outputs1, states1 = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw1, cell_bw=cell_bw1, inputs=input_batch, sequence_length=sequence_length, \\\n",
    "            initial_state_fw=init_states[0][0], initial_state_bw=init_states[0][1], dtype=tf.float32)\n",
    "        output_blstm1 = tf.concat(values=outputs1, axis=2)\n",
    "    with tf.variable_scope(\"blstm2\"):\n",
    "        cell_fw2 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units2, name='forward')\n",
    "        cell_fw2 = tf.nn.rnn_cell.DropoutWrapper(cell_fw2, output_keep_prob=droprate_output, state_keep_prob=droprate_state, variational_recurrent=True, dtype=tf.float32)\n",
    "        cell_bw2 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units2, name='backward')\n",
    "        cell_bw2 = tf.nn.rnn_cell.DropoutWrapper(cell_bw2, output_keep_prob=droprate_output, state_keep_prob=droprate_state, variational_recurrent=True, dtype=tf.float32)\n",
    "        _, states2 = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw2, cell_bw=cell_bw2, inputs=output_blstm1, sequence_length=sequence_length, \\\n",
    "            initial_state_fw=init_states[1][0], initial_state_bw=init_states[1][1], dtype=tf.float32)\n",
    "    return states1[0], states1[1], states2[0], states2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    }
   ],
   "source": [
    "# bidirectional lstms\n",
    "# Q lstms\n",
    "with tf.variable_scope('blstm_q'):\n",
    "    q_s1f, q_s1b, q_s2f, q_s2b = blstm_func(batch_input_q, batch_input_q_len)\n",
    "# A lstms\n",
    "with tf.variable_scope('blstm_a'):\n",
    "    a_s1f, a_s1b, a_s2f, a_s2b = blstm_func(batch_input_a, batch_input_a_len, ((q_s1f, q_s1b), (q_s2f, q_s2b)))\n",
    "mlp_batch_input = tf.concat(values=[a_s2f.h, a_s2b.h], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multi-layers perceptrons\n",
    "with tf.variable_scope('final_dense'):\n",
    "    dropout0 = tf.layers.dropout(mlp_batch_input, rate=drop_rate, training=enable_dropout, name='dropout0')\n",
    "    dense1 = tf.layers.dense(dropout0, dense_units1, activation=tf.nn.relu, name='dense1')\n",
    "    dropout1 = tf.layers.dropout(dense1, rate=drop_rate, training=enable_dropout, name='dropout1')\n",
    "    dense2 = tf.layers.dense(dropout1, dense_units2, activation=tf.nn.relu, name='dense2')\n",
    "    dropout2 = tf.layers.dropout(dense2, rate=drop_rate, training=enable_dropout, name='dropout2')\n",
    "    logits = tf.layers.dense(dropout2, 1, name='final_output')\n",
    "    batch_loss = tf.losses.mean_squared_error(score_label, logits)\n",
    "    tf.summary.scalar('batch_mse_loss', batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regularization term\n",
    "tv = tf.trainable_variables()\n",
    "regularization = tf.reduce_sum([tf.nn.l2_loss(v) for v in tv])\n",
    "tf.summary.scalar('regularization', regularization)\n",
    "loss_with_reg = batch_loss\n",
    "#tf.summary.scalar('loss_with_regularization', loss_with_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_with_reg, name='train_op')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = 'result_qa_wlen_conc_dropout_' + str(hidden_units1) + '_' + str(hidden_units2) + '_' +\\\n",
    "str(dense_units1) + '_' + str(dense_units2)\n",
    "log_dir = root_dir + '/log'\n",
    "tensorboard_dir = root_dir + '/tensorboard'\n",
    "cQA_train_embedding_dir = 'cQA_train_embedding_wlen.npz'\n",
    "cQA_test_embedding_dir = 'cQA_test_embedding_wlen.npz'\n",
    "epoch_num = 10\n",
    "load_model = False\n",
    "save_model = True\n",
    "print_train_info = True\n",
    "print_test_info = True\n",
    "print_train_batch = 50\n",
    "print_test_epoch = 1\n",
    "save_model_epoch = 1\n",
    "saver = tf.train.Saver()\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_generator = DataGenerator(cQA_train_embedding_dir)\n",
    "test_data_generator = DataGenerator(cQA_test_embedding_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the validation after each training epoch, so we define the validation pre-processing functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_score(matrices, log_dir):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, log_dir + \"/model.ckpt\")\n",
    "        input_q, input_a, input_q_len, input_a_len = matrices\n",
    "        scores = sess.run(logits, feed_dict={\n",
    "                batch_input_q: input_q,\n",
    "                batch_input_a: input_a,\n",
    "                batch_input_q_len: input_q_len,\n",
    "                batch_input_a_len: input_a_len,\n",
    "                enable_dropout: False})\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_driver(data_generator, log_dir, thread_len=10):\n",
    "    data_generator.init_epoch(-1, shuffle=False)\n",
    "    input_q, input_a, input_q_len, input_a_len, aug_data, label_vec, cid_list = data_generator.get_batch()\n",
    "    pred_vec = predict_score([input_q, input_a, input_q_len, input_a_len], log_dir)\n",
    "    ans = []\n",
    "    ans_human = []\n",
    "    print(label_vec.shape, pred_vec.shape)\n",
    "    for i in range(0, input_q.shape[0], thread_len):\n",
    "        tmp_rank = []\n",
    "        tmp_rank_human = []\n",
    "        for j in range(thread_len):\n",
    "            tmp_rank.append((float(pred_vec[i + j]), j, round(2 * float(label_vec[i + j]))))\n",
    "            tmp_rank_human.append((float(pred_vec[i + j]), j, round(2 * float(label_vec[i + j])), str(cid_list[i + j][0])))\n",
    "        tmp_rank.sort(reverse=True)\n",
    "        tmp_rank_human.sort(reverse=True)\n",
    "        ans.append(tmp_rank)\n",
    "        ans_human.append(tmp_rank_human)\n",
    "    return ans, ans_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAP\n",
    "class MeasurementCalculator:\n",
    "    def __init__(self, resultFromModel, threshold=1):\n",
    "        self.resultFromModel = resultFromModel\n",
    "        self.binaryResult = self.toBinaryResult(threshold)\n",
    "        \n",
    "    def toBinaryResult(self, threshold):\n",
    "        binaryResult = []\n",
    "        for eachBatch in self.resultFromModel:\n",
    "            eachBinaryResult = []\n",
    "            for eachComment in eachBatch:\n",
    "                \n",
    "                # eachComment[0] -> score\n",
    "                # eachComment[1] -> cid\n",
    "                # eachComment[2] -> label\n",
    "                currentLabel = eachComment[2]\n",
    "                if currentLabel >= threshold:\n",
    "                    eachBinaryResult.append(1)\n",
    "                else:\n",
    "                    eachBinaryResult.append(0)\n",
    "                    \n",
    "            if len(eachBinaryResult) != len(eachBatch):\n",
    "                print('binary single batch length not equal to each bathch size...')\n",
    "                return None\n",
    "            binaryResult.append(eachBinaryResult)\n",
    "        return binaryResult\n",
    "        \n",
    "    def precisionAtk(self, r, k):\n",
    "        \"\"\"Score is precision @ k\n",
    "        Relevance is binary (nonzero is relevant).\n",
    "        >>> r = [0, 0, 1]\n",
    "        >>> precisionAtk(r, 1)\n",
    "        0.0\n",
    "        >>> precisionAtk(r, 2)\n",
    "        0.0\n",
    "        >>> precisionAtk(r, 3)\n",
    "        0.33333333333333331\n",
    "        >>> precisionAtk(r, 4)\n",
    "        Traceback (most recent call last):\n",
    "            File \"<stdin>\", line 1, in ?\n",
    "        ValueError: Relevance score length < k\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Precision @ k\n",
    "        Raises:\n",
    "            ValueError: len(r) must be >= k\n",
    "        \"\"\"\n",
    "        assert k >= 1\n",
    "        r = np.asarray(r)[:k] != 0\n",
    "    #     print('processing:')\n",
    "    #     print(r)\n",
    "    \n",
    "        if r.size != k:\n",
    "            raise ValueError('Relevance score length < k')\n",
    "        return np.mean(r)\n",
    "\n",
    "    def averagePrecision(self, r):\n",
    "        \"\"\"Score is average precision (area under PR curve)\n",
    "        Relevance is binary (nonzero is relevant).\n",
    "        >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "        >>> delta_r = 1. / sum(r)\n",
    "        >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "        0.7833333333333333\n",
    "        >>> average_precision(r)\n",
    "        0.78333333333333333\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Average precision\n",
    "        \"\"\"\n",
    "        r = np.asarray(r) != 0\n",
    "        out = [self.precisionAtk(r, k + 1) for k in range(r.size) if r[k]]\n",
    "        if not out:\n",
    "            return 0.\n",
    "        return np.mean(out)\n",
    "\n",
    "#     def meanAveragePrecision(self, rs):\n",
    "    def meanAveragePrecision(self):\n",
    "        \"\"\"Score is mean average precision\n",
    "        Relevance is binary (nonzero is relevant).\n",
    "        >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "        >>> mean_average_precision(rs)\n",
    "        0.78333333333333333\n",
    "        >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "        >>> mean_average_precision(rs)\n",
    "        0.39166666666666666\n",
    "        Args:\n",
    "            rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Mean average precision\n",
    "        \"\"\"\n",
    "        rs = self.binaryResult\n",
    "        return np.mean([self.averagePrecision(r) for r in rs])\n",
    "    \n",
    "    # MRR\n",
    "#     def meanReciprocalRank(rs):\n",
    "    def meanReciprocalRank(self):\n",
    "        # Eample\n",
    "        \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "        First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "        Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "        >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "        >>> mean_reciprocal_rank(rs)\n",
    "        0.61111111111111105\n",
    "        >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "        >>> mean_reciprocal_rank(rs)\n",
    "        0.5\n",
    "        >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "        >>> mean_reciprocal_rank(rs)\n",
    "        0.75\n",
    "        Args:\n",
    "            rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Mean reciprocal rank\n",
    "        \"\"\"\n",
    "        rs = self.binaryResult\n",
    "        rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "        return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "    \n",
    "    # AveRec\n",
    "    def recallAtk(self, r, k):\n",
    "        assert k >= 1\n",
    "        totalOne = sum(r)\n",
    "        retrivedOne = sum(r[:k])\n",
    "        return retrivedOne / totalOne\n",
    "\n",
    "    def averageRecallEach(self, r):\n",
    "        #r = np.asarray(r) != 0\n",
    "        #out = [self.recallAtk(r, k + 1) for k in range(r.size) if r[k]]\n",
    "        out = [self.recallAtk(r, k + 1) for k in range(len(r)) if r[k]]\n",
    "        if not out:\n",
    "            return 0.\n",
    "        return np.mean(out)\n",
    "\n",
    "    def averageRecall(self):\n",
    "        rs = self.binaryResult\n",
    "        return np.mean([self.averageRecallEach(r) for r in rs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 batch 50: training loss 0.149885\n",
      "Epoch 0 batch 100: training loss 0.269800\n",
      "Epoch 0 batch 150: training loss 0.120062\n",
      "Epoch 0 batch 200: training loss 0.187240\n",
      "Epoch 0 batch 250: training loss 0.145395\n",
      "Epoch 0 batch 300: training loss 0.124735\n",
      "Epoch 0 batch 350: training loss 0.188034\n",
      "Epoch 0 batch 400: training loss 0.264026\n",
      "Epoch 0 batch 450: training loss 0.173756\n",
      "Epoch 0 batch 500: training loss 0.104842\n",
      "Epoch 0 batch 550: training loss 0.156751\n",
      "Epoch 0 batch 600: training loss 0.126132\n",
      "Epoch 0 batch 650: training loss 0.136384\n",
      "Epoch 0 batch 700: training loss 0.166024\n",
      "Epoch 0 batch 750: training loss 0.128984\n",
      "Epoch 0 batch 800: training loss 0.217763\n",
      "Epoch 0 batch 850: training loss 0.140879\n",
      "Epoch 0 done: training loss 0.245111\n",
      "Epoch 0 done: testing loss 0.170501\n",
      "INFO:tensorflow:Restoring parameters from result_qa_wlen_conc_dropout_64_64_128_128/log/model.ckpt\n",
      "(3270, 1) (3270, 1)\n",
      "Epoch 0 done: testing MAP 0.712585, MPR 0.789946\n",
      "Epoch 1 batch 900: training loss 0.213442\n",
      "Epoch 1 batch 950: training loss 0.170738\n",
      "Epoch 1 batch 1000: training loss 0.181129\n",
      "Epoch 1 batch 1050: training loss 0.072593\n",
      "Epoch 1 batch 1100: training loss 0.196315\n",
      "Epoch 1 batch 1150: training loss 0.220809\n",
      "Epoch 1 batch 1200: training loss 0.152712\n",
      "Epoch 1 batch 1250: training loss 0.141869\n",
      "Epoch 1 batch 1300: training loss 0.125116\n",
      "Epoch 1 batch 1350: training loss 0.142484\n",
      "Epoch 1 batch 1400: training loss 0.073003\n",
      "Epoch 1 batch 1450: training loss 0.132305\n",
      "Epoch 1 batch 1500: training loss 0.174477\n",
      "Epoch 1 batch 1550: training loss 0.146004\n",
      "Epoch 1 batch 1600: training loss 0.179332\n",
      "Epoch 1 batch 1650: training loss 0.200916\n",
      "Epoch 1 batch 1700: training loss 0.195138\n",
      "Epoch 1 batch 1750: training loss 0.158483\n",
      "Epoch 1 done: training loss 0.284260\n",
      "Epoch 1 done: testing loss 0.174397\n",
      "INFO:tensorflow:Restoring parameters from result_qa_wlen_conc_dropout_64_64_128_128/log/model.ckpt\n",
      "(3270, 1) (3270, 1)\n",
      "Epoch 1 done: testing MAP 0.728666, MPR 0.799026\n",
      "Epoch 2 batch 1800: training loss 0.144299\n",
      "Epoch 2 batch 1850: training loss 0.134537\n",
      "Epoch 2 batch 1900: training loss 0.081749\n",
      "Epoch 2 batch 1950: training loss 0.147490\n",
      "Epoch 2 batch 2000: training loss 0.071615\n",
      "Epoch 2 batch 2050: training loss 0.084186\n",
      "Epoch 2 batch 2100: training loss 0.151885\n",
      "Epoch 2 batch 2150: training loss 0.103330\n",
      "Epoch 2 batch 2200: training loss 0.198067\n",
      "Epoch 2 batch 2250: training loss 0.199273\n",
      "Epoch 2 batch 2300: training loss 0.134419\n",
      "Epoch 2 batch 2350: training loss 0.233272\n",
      "Epoch 2 batch 2400: training loss 0.107952\n",
      "Epoch 2 batch 2450: training loss 0.126279\n",
      "Epoch 2 batch 2500: training loss 0.093613\n",
      "Epoch 2 batch 2550: training loss 0.147896\n",
      "Epoch 2 batch 2600: training loss 0.057880\n",
      "Epoch 2 done: training loss 0.092976\n",
      "Epoch 2 done: testing loss 0.169522\n",
      "INFO:tensorflow:Restoring parameters from result_qa_wlen_conc_dropout_64_64_128_128/log/model.ckpt\n",
      "(3270, 1) (3270, 1)\n",
      "Epoch 2 done: testing MAP 0.745269, MPR 0.821550\n",
      "Epoch 3 batch 2650: training loss 0.200363\n",
      "Epoch 3 batch 2700: training loss 0.194072\n",
      "Epoch 3 batch 2750: training loss 0.102938\n",
      "Epoch 3 batch 2800: training loss 0.058252\n",
      "Epoch 3 batch 2850: training loss 0.082825\n",
      "Epoch 3 batch 2900: training loss 0.087401\n",
      "Epoch 3 batch 2950: training loss 0.133760\n",
      "Epoch 3 batch 3000: training loss 0.116015\n",
      "Epoch 3 batch 3050: training loss 0.128587\n",
      "Epoch 3 batch 3100: training loss 0.134515\n",
      "Epoch 3 batch 3150: training loss 0.122245\n",
      "Epoch 3 batch 3200: training loss 0.126041\n",
      "Epoch 3 batch 3250: training loss 0.264482\n",
      "Epoch 3 batch 3300: training loss 0.060290\n",
      "Epoch 3 batch 3350: training loss 0.123766\n",
      "Epoch 3 batch 3400: training loss 0.197803\n",
      "Epoch 3 batch 3450: training loss 0.133604\n",
      "Epoch 3 batch 3500: training loss 0.127364\n",
      "Epoch 3 done: training loss 0.183735\n",
      "Epoch 3 done: testing loss 0.176561\n",
      "INFO:tensorflow:Restoring parameters from result_qa_wlen_conc_dropout_64_64_128_128/log/model.ckpt\n",
      "(3270, 1) (3270, 1)\n",
      "Epoch 3 done: testing MAP 0.726059, MPR 0.791832\n",
      "Epoch 4 batch 3550: training loss 0.053870\n",
      "Epoch 4 batch 3600: training loss 0.081478\n",
      "Epoch 4 batch 3650: training loss 0.103111\n",
      "Epoch 4 batch 3700: training loss 0.062345\n",
      "Epoch 4 batch 3750: training loss 0.140445\n",
      "Epoch 4 batch 3800: training loss 0.148463\n",
      "Epoch 4 batch 3850: training loss 0.232959\n",
      "Epoch 4 batch 3900: training loss 0.145823\n",
      "Epoch 4 batch 3950: training loss 0.120886\n",
      "Epoch 4 batch 4000: training loss 0.167640\n",
      "Epoch 4 batch 4050: training loss 0.210418\n",
      "Epoch 4 batch 4100: training loss 0.133434\n",
      "Epoch 4 batch 4150: training loss 0.081115\n",
      "Epoch 4 batch 4200: training loss 0.083486\n",
      "Epoch 4 batch 4250: training loss 0.082953\n",
      "Epoch 4 batch 4300: training loss 0.065950\n",
      "Epoch 4 batch 4350: training loss 0.154405\n",
      "Epoch 4 batch 4400: training loss 0.071424\n",
      "Epoch 4 done: training loss 0.123636\n",
      "Epoch 4 done: testing loss 0.164433\n",
      "INFO:tensorflow:Restoring parameters from result_qa_wlen_conc_dropout_64_64_128_128/log/model.ckpt\n",
      "(3270, 1) (3270, 1)\n",
      "Epoch 4 done: testing MAP 0.731824, MPR 0.802355\n",
      "Epoch 5 batch 4450: training loss 0.069915\n",
      "Epoch 5 batch 4500: training loss 0.199608\n",
      "Epoch 5 batch 4550: training loss 0.128531\n",
      "Epoch 5 batch 4600: training loss 0.141094\n",
      "Epoch 5 batch 4650: training loss 0.068452\n",
      "Epoch 5 batch 4700: training loss 0.142553\n",
      "Epoch 5 batch 4750: training loss 0.057698\n",
      "Epoch 5 batch 4800: training loss 0.111514\n",
      "Epoch 5 batch 4850: training loss 0.053004\n",
      "Epoch 5 batch 4900: training loss 0.128145\n",
      "Epoch 5 batch 4950: training loss 0.070385\n",
      "Epoch 5 batch 5000: training loss 0.093894\n",
      "Epoch 5 batch 5050: training loss 0.128817\n",
      "Epoch 5 batch 5100: training loss 0.093698\n",
      "Epoch 5 batch 5150: training loss 0.108518\n",
      "Epoch 5 batch 5200: training loss 0.066506\n",
      "Epoch 5 batch 5250: training loss 0.107081\n",
      "Epoch 5 done: training loss 0.259100\n",
      "Epoch 5 done: testing loss 0.177944\n",
      "INFO:tensorflow:Restoring parameters from result_qa_wlen_conc_dropout_64_64_128_128/log/model.ckpt\n",
      "(3270, 1) (3270, 1)\n",
      "Epoch 5 done: testing MAP 0.728000, MPR 0.798783\n",
      "Epoch 6 batch 5300: training loss 0.112014\n",
      "Epoch 6 batch 5350: training loss 0.045354\n",
      "Epoch 6 batch 5400: training loss 0.113393\n",
      "Epoch 6 batch 5450: training loss 0.103096\n",
      "Epoch 6 batch 5500: training loss 0.061537\n",
      "Epoch 6 batch 5550: training loss 0.174000\n",
      "Epoch 6 batch 5600: training loss 0.115993\n",
      "Epoch 6 batch 5650: training loss 0.115362\n",
      "Epoch 6 batch 5700: training loss 0.118372\n",
      "Epoch 6 batch 5750: training loss 0.106586\n",
      "Epoch 6 batch 5800: training loss 0.174774\n",
      "Epoch 6 batch 5850: training loss 0.083145\n",
      "Epoch 6 batch 5900: training loss 0.083221\n",
      "Epoch 6 batch 5950: training loss 0.106696\n",
      "Epoch 6 batch 6000: training loss 0.209047\n",
      "Epoch 6 batch 6050: training loss 0.196736\n",
      "Epoch 6 batch 6100: training loss 0.132782\n",
      "Epoch 6 batch 6150: training loss 0.024934\n",
      "Epoch 6 done: training loss 0.076833\n",
      "Epoch 6 done: testing loss 0.174992\n",
      "INFO:tensorflow:Restoring parameters from result_qa_wlen_conc_dropout_64_64_128_128/log/model.ckpt\n",
      "(3270, 1) (3270, 1)\n",
      "Epoch 6 done: testing MAP 0.718106, MPR 0.784411\n",
      "Epoch 7 batch 6200: training loss 0.032259\n",
      "Epoch 7 batch 6250: training loss 0.070589\n",
      "Epoch 7 batch 6300: training loss 0.059753\n",
      "Epoch 7 batch 6350: training loss 0.053394\n",
      "Epoch 7 batch 6400: training loss 0.179203\n",
      "Epoch 7 batch 6450: training loss 0.054152\n",
      "Epoch 7 batch 6500: training loss 0.057870\n",
      "Epoch 7 batch 6550: training loss 0.072720\n",
      "Epoch 7 batch 6600: training loss 0.050267\n",
      "Epoch 7 batch 6650: training loss 0.113934\n",
      "Epoch 7 batch 6700: training loss 0.078061\n",
      "Epoch 7 batch 6750: training loss 0.120220\n",
      "Epoch 7 batch 6800: training loss 0.065169\n",
      "Epoch 7 batch 6850: training loss 0.073629\n",
      "Epoch 7 batch 6900: training loss 0.057653\n",
      "Epoch 7 batch 6950: training loss 0.043700\n",
      "Epoch 7 batch 7000: training loss 0.093448\n",
      "Epoch 7 batch 7050: training loss 0.067211\n",
      "Epoch 7 done: training loss 0.117038\n",
      "Epoch 7 done: testing loss 0.175980\n",
      "INFO:tensorflow:Restoring parameters from result_qa_wlen_conc_dropout_64_64_128_128/log/model.ckpt\n",
      "(3270, 1) (3270, 1)\n",
      "Epoch 7 done: testing MAP 0.728559, MPR 0.798528\n",
      "Epoch 8 batch 7100: training loss 0.109463\n",
      "Epoch 8 batch 7150: training loss 0.052332\n",
      "Epoch 8 batch 7200: training loss 0.044408\n",
      "Epoch 8 batch 7250: training loss 0.042623\n",
      "Epoch 8 batch 7300: training loss 0.067854\n",
      "Epoch 8 batch 7350: training loss 0.066205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 batch 7400: training loss 0.055139\n",
      "Epoch 8 batch 7450: training loss 0.056004\n",
      "Epoch 8 batch 7500: training loss 0.066110\n",
      "Epoch 8 batch 7550: training loss 0.035958\n",
      "Epoch 8 batch 7600: training loss 0.067032\n",
      "Epoch 8 batch 7650: training loss 0.041874\n",
      "Epoch 8 batch 7700: training loss 0.032068\n",
      "Epoch 8 batch 7750: training loss 0.100234\n",
      "Epoch 8 batch 7800: training loss 0.092916\n",
      "Epoch 8 batch 7850: training loss 0.093039\n",
      "Epoch 8 batch 7900: training loss 0.107577\n",
      "Epoch 8 done: training loss 0.014886\n",
      "Epoch 8 done: testing loss 0.179179\n",
      "INFO:tensorflow:Restoring parameters from result_qa_wlen_conc_dropout_64_64_128_128/log/model.ckpt\n",
      "(3270, 1) (3270, 1)\n",
      "Epoch 8 done: testing MAP 0.727592, MPR 0.801897\n",
      "Epoch 9 batch 7950: training loss 0.071338\n",
      "Epoch 9 batch 8000: training loss 0.123138\n",
      "Epoch 9 batch 8050: training loss 0.070864\n",
      "Epoch 9 batch 8100: training loss 0.112731\n",
      "Epoch 9 batch 8150: training loss 0.100497\n",
      "Epoch 9 batch 8200: training loss 0.128364\n",
      "Epoch 9 batch 8250: training loss 0.117148\n",
      "Epoch 9 batch 8300: training loss 0.092060\n",
      "Epoch 9 batch 8350: training loss 0.124591\n",
      "Epoch 9 batch 8400: training loss 0.041042\n",
      "Epoch 9 batch 8450: training loss 0.029683\n",
      "Epoch 9 batch 8500: training loss 0.129563\n",
      "Epoch 9 batch 8550: training loss 0.056550\n",
      "Epoch 9 batch 8600: training loss 0.086451\n",
      "Epoch 9 batch 8650: training loss 0.044563\n",
      "Epoch 9 batch 8700: training loss 0.105345\n",
      "Epoch 9 batch 8750: training loss 0.109065\n",
      "Epoch 9 batch 8800: training loss 0.112225\n",
      "Epoch 9 done: training loss 0.104902\n",
      "Epoch 9 done: testing loss 0.180155\n",
      "INFO:tensorflow:Restoring parameters from result_qa_wlen_conc_dropout_64_64_128_128/log/model.ckpt\n",
      "(3270, 1) (3270, 1)\n",
      "Epoch 9 done: testing MAP 0.716204, MPR 0.786501\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)\n",
    "    if load_model:\n",
    "        saver.restore(sess, log_dir + \"/model.ckpt\")\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    batch_i = 0\n",
    "    for epoch_i in range(epoch_num):\n",
    "        train_data_generator.init_epoch(batch_size)\n",
    "        while train_data_generator.has_next():\n",
    "            input_q, input_a, input_q_len, input_a_len, aug_data, labels, _ = train_data_generator.get_batch()\n",
    "            _, summary_log, current_batch_loss = sess.run([train_op, summary_op, batch_loss], feed_dict={\n",
    "                batch_input_q: input_q,\n",
    "                batch_input_a: input_a,\n",
    "                batch_input_q_len: input_q_len,\n",
    "                batch_input_a_len: input_a_len,\n",
    "                score_label: labels,\n",
    "                enable_dropout: True})\n",
    "            batch_i += 1\n",
    "            if print_train_info and batch_i % print_train_batch == 0:\n",
    "                writer.add_summary(summary_log, batch_i)\n",
    "                print('Epoch %d batch %d: training loss %f' % (epoch_i, batch_i, current_batch_loss.item()))\n",
    "        print('Epoch %d done: training loss %f' % (epoch_i, current_batch_loss.item()))\n",
    "        if print_test_info and epoch_i % print_test_epoch == 0:\n",
    "            test_data_generator.init_epoch(-1, False)\n",
    "            input_q, input_a, input_q_len, input_a_len, aug_data, labels, _ = test_data_generator.get_batch()\n",
    "            test_batch_loss = sess.run([batch_loss], feed_dict={\n",
    "                batch_input_q: input_q,\n",
    "                batch_input_a: input_a,\n",
    "                batch_input_q_len: input_q_len,\n",
    "                batch_input_a_len: input_a_len,\n",
    "                score_label: labels,\n",
    "                enable_dropout: False})\n",
    "            print('Epoch %d done: testing loss %f' % (epoch_i, test_batch_loss[0].item()))\n",
    "        if save_model and epoch_i % save_model_epoch == 0:\n",
    "            save_path = saver.save(sess, log_dir + \"/model.ckpt\")\n",
    "            ranks, _ = test_driver(test_data_generator, log_dir)\n",
    "            mc = MeasurementCalculator(ranks, 2)\n",
    "            print('Epoch %d done: testing MAP %f, MPR %f' % (epoch_i, mc.meanAveragePrecision(), mc.meanReciprocalRank()))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the running above, we did the validation after each epoch. We notice the highest validation performance achieved after the 2nd epoch, which is MAP: 74.52% and MRR: 82.15%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
