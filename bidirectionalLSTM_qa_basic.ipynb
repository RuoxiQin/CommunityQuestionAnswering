{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic bidirectional LSTM model on Answer Selection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_length = 100\n",
    "vector_size = 300\n",
    "batch_size = 16\n",
    "hidden_units1 = 64\n",
    "hidden_units2 = 32\n",
    "learning_rate = 0.001\n",
    "drop_rate = 0.5\n",
    "augment_feature_num = 10\n",
    "dense_units1 = 64\n",
    "dense_units2 = 32\n",
    "reg_coefficient = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "You can impelment whatever you want, just make sure to have the *has_next* and *get_batch* function with the corresponding output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, data_dir):\n",
    "        '''Initialization'''\n",
    "        npz_data = np.load(data_dir)\n",
    "        names = sorted(npz_data.files)\n",
    "        self._data = []\n",
    "        for name in names:\n",
    "            self._data.append(npz_data[name])\n",
    "        self._num_examples = self._data[0].shape[0]\n",
    "\n",
    "    def shuffle_data(self, idx):\n",
    "        for i in range(len(self._data)):\n",
    "            self._data[i]=self._data[i][idx]\n",
    "\n",
    "    def get_data(self, start, end):\n",
    "        res=[]\n",
    "        for i in range(len(self._data)):\n",
    "            res.append(self._data[i][start : end])\n",
    "        return tuple(res)\n",
    "    \n",
    "    def init_epoch(self, batch_size, shuffle=True):\n",
    "        self._index_in_epoch = 0\n",
    "        idx = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx)  # shuffle indexes\n",
    "        self.shuffle_data(idx)  # get list of `num` random samples\n",
    "        if batch_size <= 0:\n",
    "            self.batch_size = self._num_examples\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "    def has_next(self):\n",
    "        '''return bool: whether there is a next batch'''\n",
    "        return self._index_in_epoch < self._num_examples\n",
    "\n",
    "    def get_batch(self):\n",
    "        '''\n",
    "        return the next batch in the following tuple format:\n",
    "        (batch_input_q, batch_input_a, augmented_data, score_label)\n",
    "        Where\n",
    "        batch_input_q: word2vec representation for the question in shape [batch_size, sentence_length, vector_size]\n",
    "        batch_input_a: word2vec representation for the answer in shape [batch_size, sentence_length, vector_size]\n",
    "        augmented_data: the extra data for MLP in shape [batch_size, augment_feature_num]\n",
    "        score_label: ground truth semantic similarity score in shape [batch_size, 1]\n",
    "        comment_id: the commet id\n",
    "        '''\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += self.batch_size\n",
    "        end = self._index_in_epoch\n",
    "        return self.get_data(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholders\n",
    "batch_input_q = tf.placeholder(dtype=tf.float32, shape=(None, sentence_length, vector_size))\n",
    "batch_input_a = tf.placeholder(dtype=tf.float32, shape=(None, sentence_length, vector_size))\n",
    "augmented_data = tf.placeholder(dtype=tf.float32, shape=(None, augment_feature_num))\n",
    "score_label = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "enable_dropout = tf.placeholder(dtype=tf.bool, shape=())\n",
    "batch_input = tf.concat(values=[batch_input_q, batch_input_a], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bidirectional lstms\n",
    "cell_fw1 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units1, name='forward1')\n",
    "cell_bw1 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units1, name='backward1')\n",
    "outputs1, states1 = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=cell_fw1, cell_bw=cell_bw1, inputs=batch_input, dtype=tf.float32)\n",
    "batch_middle = tf.concat(values=outputs1, axis=2)\n",
    "cell_fw2 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units2, name='forward2')\n",
    "cell_bw2 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units2, name='backward2')\n",
    "outputs2, states2 = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=cell_fw2, cell_bw=cell_bw2, inputs=batch_middle, dtype=tf.float32)\n",
    "output_fw2, output_bw2 = outputs2\n",
    "bilstm_output = output_fw2 + output_bw2\n",
    "bilstm_flaten = tf.reshape(bilstm_output, (-1, hidden_units2 * sentence_length * 2))\n",
    "mlp_batch_input = tf.concat(values=[bilstm_flaten, augmented_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'batch_mse_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi-layers perceptrons\n",
    "dropout1 = tf.layers.dropout(mlp_batch_input, rate=drop_rate, training=enable_dropout, name='dropout1')\n",
    "dense1 = tf.layers.dense(dropout1, dense_units1, activation=tf.nn.relu, name='dense1')\n",
    "dropout2 = tf.layers.dropout(dense1, rate=drop_rate, training=enable_dropout, name='dropout2')\n",
    "dense2 = tf.layers.dense(dropout2, dense_units2, activation=tf.nn.relu, name='dense2')\n",
    "dropout3 = tf.layers.dropout(dense2, rate=drop_rate, training=enable_dropout, name='dropout3')\n",
    "logits = tf.layers.dense(dropout3, 1, name='final_output')\n",
    "batch_loss = tf.losses.mean_squared_error(score_label, logits)\n",
    "tf.summary.scalar('batch_mse_loss', batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_with_regularization:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regularization term\n",
    "tv = tf.trainable_variables()\n",
    "regularization = tf.reduce_sum([tf.nn.l2_loss(v) for v in tv])\n",
    "tf.summary.scalar('regularization', regularization)\n",
    "loss_with_reg = batch_loss + reg_coefficient * regularization\n",
    "tf.summary.scalar('loss_with_regularization', loss_with_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_with_reg, name='train_op')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = 'result_qa_basic_' + str(hidden_units1) + '_' + str(hidden_units2) + '_' +\\\n",
    "str(dense_units1) + '_' + str(dense_units2) + '_' + str(reg_coefficient)\n",
    "log_dir = root_dir + '/log'\n",
    "tensorboard_dir = root_dir + '/tensorboard'\n",
    "cQA_train_embedding_dir = 'cQA_train_embedding.npz'\n",
    "cQA_test_embedding_dir = 'cQA_test_embedding.npz'\n",
    "epoch_num = 10\n",
    "load_model = False\n",
    "save_model = True\n",
    "print_train_info = True\n",
    "print_test_info = True\n",
    "print_train_batch = 50\n",
    "print_test_epoch = 1\n",
    "save_model_epoch = 1\n",
    "saver = tf.train.Saver()\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_generator = DataGenerator(cQA_train_embedding_dir)\n",
    "test_data_generator = DataGenerator(cQA_test_embedding_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 batch 50: training loss 0.277439\n",
      "Epoch 0 batch 100: training loss 0.153597\n",
      "Epoch 0 batch 150: training loss 0.234903\n",
      "Epoch 0 batch 200: training loss 0.097543\n",
      "Epoch 0 batch 250: training loss 0.116364\n",
      "Epoch 0 batch 300: training loss 0.318267\n",
      "Epoch 0 batch 350: training loss 0.231397\n",
      "Epoch 0 batch 400: training loss 0.155965\n",
      "Epoch 0 batch 450: training loss 0.171484\n",
      "Epoch 0 batch 500: training loss 0.137690\n",
      "Epoch 0 batch 550: training loss 0.182077\n",
      "Epoch 0 batch 600: training loss 0.252278\n",
      "Epoch 0 batch 650: training loss 0.247577\n",
      "Epoch 0 batch 700: training loss 0.182637\n",
      "Epoch 0 batch 750: training loss 0.204921\n",
      "Epoch 0 batch 800: training loss 0.186273\n",
      "Epoch 0 batch 850: training loss 0.217641\n",
      "Epoch 0 done: training loss 0.127808\n",
      "Epoch 0 done: testing loss 0.181902\n",
      "Epoch 1 batch 900: training loss 0.189113\n",
      "Epoch 1 batch 950: training loss 0.204325\n",
      "Epoch 1 batch 1000: training loss 0.168804\n",
      "Epoch 1 batch 1050: training loss 0.190128\n",
      "Epoch 1 batch 1100: training loss 0.186275\n",
      "Epoch 1 batch 1150: training loss 0.151166\n",
      "Epoch 1 batch 1200: training loss 0.174782\n",
      "Epoch 1 batch 1250: training loss 0.110060\n",
      "Epoch 1 batch 1300: training loss 0.203082\n",
      "Epoch 1 batch 1350: training loss 0.222690\n",
      "Epoch 1 batch 1400: training loss 0.167136\n",
      "Epoch 1 batch 1450: training loss 0.168962\n",
      "Epoch 1 batch 1500: training loss 0.188702\n",
      "Epoch 1 batch 1550: training loss 0.164817\n",
      "Epoch 1 batch 1600: training loss 0.154923\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)\n",
    "    if load_model:\n",
    "        saver.restore(sess, log_dir + \"/model.ckpt\")\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    batch_i = 0\n",
    "    for epoch_i in range(epoch_num):\n",
    "        train_data_generator.init_epoch(batch_size)\n",
    "        while train_data_generator.has_next():\n",
    "            input_q, input_a, aug_data, labels = train_data_generator.get_batch()\n",
    "            _, summary_log, current_batch_loss = sess.run([train_op, summary_op, batch_loss], feed_dict={\n",
    "                batch_input_q: input_q,\n",
    "                batch_input_a: input_a,\n",
    "                augmented_data: aug_data,\n",
    "                score_label: labels,\n",
    "                enable_dropout: True})\n",
    "            batch_i += 1\n",
    "            if print_train_info and batch_i % print_train_batch == 0:\n",
    "                writer.add_summary(summary_log, batch_i)\n",
    "                print('Epoch %d batch %d: training loss %f' % (epoch_i, batch_i, current_batch_loss.item()))\n",
    "        print('Epoch %d done: training loss %f' % (epoch_i, current_batch_loss.item()))\n",
    "        if print_test_info and epoch_i % print_test_epoch == 0:\n",
    "            test_data_generator.init_epoch(-1, False)\n",
    "            input_q, input_a, aug_data, labels = test_data_generator.get_batch()\n",
    "            test_batch_loss = sess.run([batch_loss], feed_dict={\n",
    "                batch_input_q: input_q,\n",
    "                batch_input_a: input_a,\n",
    "                augmented_data: aug_data,\n",
    "                score_label: labels,\n",
    "                enable_dropout: False})\n",
    "            print('Epoch %d done: testing loss %f' % (epoch_i, test_batch_loss[0].item()))\n",
    "        if save_model and epoch_i % save_model_epoch == 0:\n",
    "            save_path = saver.save(sess, log_dir + \"/model.ckpt\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
