{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = 100\n",
    "vector_size = 300\n",
    "batch_size = 16\n",
    "hidden_units1 = 64\n",
    "hidden_units2 = 32\n",
    "learning_rate = 0.001\n",
    "drop_rate = 0.5\n",
    "augment_feature_num = 1\n",
    "dense_units1 = 128\n",
    "dense_units2 = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "You can impelment whatever you want, just make sure to have the *has_next* and *get_batch* function with the corresponding output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, data_dir):\n",
    "        '''Initialization'''\n",
    "        npz_data = np.load(data_dir)\n",
    "        names = sorted(npz_data.files)\n",
    "        self._data = []\n",
    "        for name in names:\n",
    "            self._data.append(npz_data[name])\n",
    "        self._num_examples = self._data[0].shape[0]\n",
    "\n",
    "    def shuffle_data(self, idx):\n",
    "        for i in range(len(self._data)):\n",
    "            self._data[i]=self._data[i][idx]\n",
    "\n",
    "    def get_data(self, start, end):\n",
    "        res=[]\n",
    "        for i in range(len(self._data)):\n",
    "            res.append(self._data[i][start : end])\n",
    "        return tuple(res)\n",
    "    \n",
    "    def init_epoch(self, batch_size, shuffle=True):\n",
    "        self._index_in_epoch = 0\n",
    "        idx = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx)  # shuffle indexes\n",
    "        self.shuffle_data(idx)  # get list of `num` random samples\n",
    "        if batch_size <= 0:\n",
    "            self.batch_size = self._num_examples\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "    def has_next(self):\n",
    "        '''return bool: whether there is a next batch'''\n",
    "        return self._index_in_epoch < self._num_examples\n",
    "\n",
    "    def get_batch(self):\n",
    "        '''\n",
    "        return the next batch in the following tuple format:\n",
    "        (batch_input_q, batch_input_a, augmented_data, score_label)\n",
    "        Where\n",
    "        batch_input_q: word2vec representation for the question in shape [batch_size, sentence_length, vector_size]\n",
    "        batch_input_a: word2vec representation for the answer in shape [batch_size, sentence_length, vector_size]\n",
    "        augmented_data: the extra data for MLP in shape [batch_size, augment_feature_num]\n",
    "        score_label: ground truth semantic similarity score in shape [batch_size, 1]\n",
    "        '''\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += self.batch_size\n",
    "        end = self._index_in_epoch\n",
    "        return self.get_data(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders\n",
    "batch_input_q = tf.placeholder(dtype=tf.float32, shape=(None, sentence_length, vector_size))\n",
    "batch_input_a = tf.placeholder(dtype=tf.float32, shape=(None, sentence_length, vector_size))\n",
    "augmented_data = tf.placeholder(dtype=tf.float32, shape=(None, augment_feature_num))\n",
    "score_label = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "enable_dropout = tf.placeholder(dtype=tf.bool, shape=())\n",
    "batch_input = tf.concat(values=[batch_input_q, batch_input_a], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional lstms\n",
    "cell_fw1 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units1, name='forward1')\n",
    "cell_bw1 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units1, name='backward1')\n",
    "outputs1, states1 = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=cell_fw1, cell_bw=cell_bw1, inputs=batch_input, dtype=tf.float32)\n",
    "batch_middle = tf.concat(values=outputs1, axis=2)\n",
    "cell_fw2 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units2, name='forward2')\n",
    "cell_bw2 = tf.nn.rnn_cell.LSTMCell(num_units=hidden_units2, name='backward2')\n",
    "outputs2, states2 = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=cell_fw2, cell_bw=cell_bw2, inputs=batch_middle, dtype=tf.float32)\n",
    "output_fw2, output_bw2 = outputs2\n",
    "bilstm_output = output_fw2 + output_bw2\n",
    "bilstm_flaten = tf.reshape(bilstm_output, (-1, hidden_units2 * sentence_length * 2))\n",
    "mlp_batch_input = tf.concat(values=[bilstm_flaten, augmented_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'batch_mse_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi-layers perceptrons\n",
    "dropout1 = tf.layers.dropout(mlp_batch_input, rate=drop_rate, training=enable_dropout, name='dropout1')\n",
    "dense1 = tf.layers.dense(dropout1, dense_units1, activation=tf.nn.relu, name='dense1')\n",
    "dropout2 = tf.layers.dropout(dense1, rate=drop_rate, training=enable_dropout, name='dropout2')\n",
    "dense2 = tf.layers.dense(dropout2, dense_units2, activation=tf.nn.relu, name='dense2')\n",
    "dropout3 = tf.layers.dropout(dense2, rate=drop_rate, training=enable_dropout, name='dropout3')\n",
    "logits = tf.layers.dense(dropout3, 1, name='final_output')\n",
    "batch_loss = tf.losses.mean_squared_error(score_label, logits)\n",
    "tf.summary.scalar('batch_mse_loss', batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_with_regularization:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regularization term\n",
    "tv = tf.trainable_variables()\n",
    "regularization = tf.reduce_sum([tf.nn.l2_loss(v) for v in tv])\n",
    "tf.summary.scalar('regularization', regularization)\n",
    "loss_with_reg = batch_loss + regularization\n",
    "tf.summary.scalar('loss_with_regularization', loss_with_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_with_reg, name='train_op')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'log_64_32_128_32'\n",
    "tensorboard_dir = 'tensorboard'\n",
    "cQA_train_embedding_dir = 'cQA_train_embedding.npz'\n",
    "cQA_test_embedding_dir = 'cQA_test_embedding.npz'\n",
    "epoch_num = 1\n",
    "load_model = True\n",
    "save_model = False\n",
    "print_train_info = True\n",
    "print_test_info = True\n",
    "print_train_batch = 20\n",
    "print_test_epoch = 1\n",
    "save_model_epoch = 1\n",
    "saver = tf.train.Saver()\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log_64_32_128_32/model.ckpt\n",
      "8902.852\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, log_dir + \"/model.ckpt\")\n",
    "    res = sess.run(regularization)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = DataGenerator(cQA_train_embedding_dir)\n",
    "test_data_generator = DataGenerator(cQA_test_embedding_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14110, 100, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_generator._data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, log_dir + \"/model.ckpt\")\n",
    "    test_data_generator.init_epoch(-1, False)\n",
    "    input_q, input_a, aug_data, labels = test_data_generator.get_batch()\n",
    "    test_batch_loss = sess.run([batch_loss], feed_dict={\n",
    "        batch_input_q: input_q,\n",
    "        batch_input_a: input_a,\n",
    "        augmented_data: aug_data,\n",
    "        score_label: labels,\n",
    "        enable_dropout: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 batch 20: training loss 0.139585\n",
      "Epoch 0 batch 40: training loss 0.240153\n",
      "Epoch 0 batch 60: training loss 0.188243\n",
      "Epoch 0 batch 80: training loss 0.205591\n",
      "Epoch 0 batch 100: training loss 0.171620\n",
      "Epoch 0 batch 120: training loss 0.151209\n",
      "Epoch 0 batch 140: training loss 0.216156\n",
      "Epoch 0 batch 160: training loss 0.252703\n",
      "Epoch 0 batch 180: training loss 0.155523\n",
      "Epoch 0 batch 200: training loss 0.152540\n",
      "Epoch 0 batch 220: training loss 0.175155\n",
      "Epoch 0 batch 240: training loss 0.206150\n",
      "Epoch 0 batch 260: training loss 0.166518\n",
      "Epoch 0 batch 280: training loss 0.227060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-24c1114a46aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0maugmented_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maug_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mscore_label\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 enable_dropout: True})\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mbatch_i\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprint_train_info\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_train_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)\n",
    "    if load_model:\n",
    "        saver.restore(sess, log_dir + \"/model.ckpt\")\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    batch_i = 0\n",
    "    for epoch_i in range(epoch_num):\n",
    "        train_data_generator.init_epoch(batch_size)\n",
    "        while train_data_generator.has_next():\n",
    "            input_q, input_a, aug_data, labels = train_data_generator.get_batch()\n",
    "            _, summary_log, current_batch_loss = sess.run([train_op, summary_op, batch_loss], feed_dict={\n",
    "                batch_input_q: input_q,\n",
    "                batch_input_a: input_a,\n",
    "                augmented_data: aug_data,\n",
    "                score_label: labels,\n",
    "                enable_dropout: True})\n",
    "            batch_i += 1\n",
    "            if print_train_info and batch_i % print_train_batch == 0:\n",
    "                writer.add_summary(summary_log, batch_i)\n",
    "                print('Epoch %d batch %d: training loss %f' % (epoch_i, batch_i, current_batch_loss.item()))\n",
    "        print('Epoch %d done: training loss %f' % (epoch_i, current_batch_loss.item()))\n",
    "        if print_test_info and epoch_i % print_test_epoch == 0:\n",
    "            test_data_generator.init_epoch(-1, False)\n",
    "            input_q, input_a, aug_data, labels = test_data_generator.get_batch()\n",
    "            test_batch_loss = sess.run([batch_loss], feed_dict={\n",
    "                batch_input_q: input_q,\n",
    "                batch_input_a: input_a,\n",
    "                augmented_data: aug_data,\n",
    "                score_label: labels,\n",
    "                enable_dropout: False})\n",
    "            print('Epoch %d done: testing loss %f' % (epoch_i, test_batch_loss[0].item()))\n",
    "        if save_model and epoch_i % save_model_epoch == 0:\n",
    "            save_path = saver.save(sess, log_dir + \"/model.ckpt\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
